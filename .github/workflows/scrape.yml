name: Scrape City Auslastung
on:
  schedule:
    - cron: "*/15 * * * *"   # every 15 minutes (UTC)
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: scrape_city_csv
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"                   # enables built-in pip cache
          cache-dependency-path: |
            requirements.txt

      # Cache Playwright browser binaries (~/.cache/ms-playwright)
      - name: Cache Playwright browsers
        uses: actions/cache@v3
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-browsers
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Install Chromium *once* (subsequent runs reuse cache above)
      - name: Ensure Chromium installed
        run: |
          python -m playwright install chromium
          # If fonts/locale deps ever missing, switch to:
          # python -m playwright install --with-deps chromium

      - name: Run scraper
        run: |
          export SCRAPE_DEBUG=1
          python remote_scrape.py

      - name: Upload debug artifacts (if any)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-artifacts
          path: artifacts/*
          if-no-files-found: ignore

      - name: Commit & push CSV
        run: |
          git config user.name "github-actions"
          git config user.email "actions@users.noreply.github.com"
          git add swiftbar_city_auslastung.csv
          git commit -m "Update CSV $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push
